## <span id="jump0">目录<span>
  
  * [决策树 Decision Tree](#jump1)
  * [item2](#jump2)
  * [item3](#jump3)
  * [item4](#jump4)
  * [item5](#jump5)

## <span id="jump1">决策树 Decision Tree<span>

  ### 框架图
 
  ### fitting metrics
  
  #### Gini impurity
  
  * When best split, Gini impurity(Entropy) reaches the minimum zero
  * ![](http://latex.codecogs.com/gif.latex?p_{i}) is the fraction of class i in this node
  * Used by the CART

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?I_{G}\left(p\right)=\sum_{i=1}^{J}\left(p_{i}\sum_{k\neq{i}}^{}p_{k}\right)">
<p>
    
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?\sum_{k\neq{j}}^{}p_{k}=1-p_{i}">
<p>
  
  #### Information gain
  
  * When best split, Entropy reaches the minimum, Information gain reaches the maximum
  * Used by the ID3, C4.5 and C5.0
  * Entropy ![](http://latex.codecogs.com/gif.latex?H\left(T\right))
  * Information gain ![](http://latex.codecogs.com/gif.latex?IG\left(T,a\right)) for every node ![](http://latex.codecogs.com/gif.latex?a)
  * The expected information gain with respect to ![](http://latex.codecogs.com/gif.latex?a) is the information gain for the split
  * Entropy is sacle free, so u can compare parent and children entropy directly to attain the information gain regardless different numbers of items in two groups

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?H\left(T\right)=I_{E}\left(p_{1},p_{2},...p_{J}\right)=-\sum_{i=1}^{J}p_{i}log_{2}p_{i}">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?IG\left(T,a\right)=H\left(T\right)-H\left(T|a\right)">
<p>
  
  ### fitting algorithm
  
  ### goodness of fit
  
  
[返回目录](#jump0)


## <span id="jump2">item2<span>
  
  ### item2.1
 
  ### item2.2
  
 
[返回目录](#jump0)

## <span id="jump3">item3<span>
  
  ### item3.1
 
  ### item3.2

[返回目录](#jump0)

## <span id="jump4">item4<span>
  
  ### item4.1
 
  ### item4.2

[返回目录](#jump0)


## <span id="jump5">item5<span>
  
  ### item5.1
 
  ### item5.2
  
[返回目录](#jump0)
