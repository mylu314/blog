## <span id="jump0">目录<span>
  
  * [随机森林](#jump1)
  * [Maven 从已有项目生成原型 `mvn archetype:create-from-project`](#jump2)
  * [修改大型项目的项目名、包名](#jump3)
  * [github 插入数学公式](#jump4)
  * [item5](#jump5)

## <span id="jump1">随机森林 Random Forest<span>
  
  ### 参考资料
  * [论文](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
  * [重点知识](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#:~:text=Random%20Forests%28tm%29%20is%20a%20trademark%20of%20Leo%20Breiman,also%20include%20RF%28tm%29%2C%20RandomForests%28tm%29%2C%20RandomForest%28tm%29%20and%20Random%20Forest%28tm%29.)

  ### 随机森林之前的一些研究思路
  * bagging
<p align="center">
  <img src="https://github.com/mylu314/blog/blob/main/images/Ensemble_Bagging.png" width=400/>
<p>
  
  * boosting
<p align="center">
  <img src="https://github.com/mylu314/blog/blob/main/images/Ensemble_Boosting.png" width=400/>
<p>
  
  * cross validation（bagging 就产生了 multiple rounds of CV 的效果）
      * less bias
      * more variance

  ### 核心思路
  * 本质：bagging + random features selection
  * 每一个综合许多决策树的判断结果
  * 相比最简单的树模型，以及最初的 bagging，boosting 思想，随机森林以及 adbbost 等更强的算法主要提高角度：
      * 增加分类器强度（strength）
          > in random forest, bagging used
      * 降低分类器间的相关性（correlation）
          > in random forest, features randomly selected used
  * 样本容量 N
  * 样本特征量 M

  ### 关键的定义
  * 分类器 classifier *h*：算法是通过分类器来实现决策判断的，在这里是「树」。
    * 数量为 k
  * 随机向量（random choices）random vector ![](http://latex.codecogs.com/gif.latex?\Theta) ：分离器 *h* 是通过样本 *x* 和随机向量 ![](http://latex.codecogs.com/gif.latex?\Theta) 共同唯一决定的。随机向量决定了分类器的结构。
      * 随机向量 i.i.d 的理解：所有可能的随机选择结果都是基于样本的特征空间，所以随机向量的分布就来源于该特征空间。不过原论文中并没有直接称其为特征或者来源于特征空间，而用了更广义的随机向量，此处不是很理解，目前只感觉特征是最好的描述方式。
  * margin function: margin 越大说明这一系列树（也就是森林）的分类结果越可信。
      * av 是因为对所有的 classifiers 取 average
      * 此处 margin 的定义和后面的本质相同

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?mg\left(X,Y\right)=av_{k}I\left(h_{k}\left(X\right)=Y\right)-max_{j\neq"/>
  <img src="http://latex.codecogs.com/gif.latex?_{Y}av_{k}I\left(h_{k}\left(X\right)=j\right)"/>
<p>
  
  * raw margin function
    * ![](http://latex.codecogs.com/gif.latex?\Theta) 在一个 classifier 中最容易理解的方式是当作特征，也就一对一对应了树
    * raw margin 于是表示每个分类器（树）自身的分类能力
    * 从随机向量和期望的数学关系很容易理解 margin 是 raw margin 的期望，也就是森林的能力是所有树能力的平均水平。

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?rmg\left(\Theta,X,Y\right)=I\left(h\left(X,\Theta\right)=Y\right)-I\left(h\left(X,\Theta\right)=\hat{j}\left(X,Y\right)\right)">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?\hat{j}\left(X,Y\right)=argmax_{j\neq{Y}}P_{\Theta}\left(h\left(X,\Theta\right)=j\right)">
<p>
  
  * correlation
    * correlation 的定义基于 rmg
    * ![](http://latex.codecogs.com/gif.latex?\bar{\rho}) 描述了整个森林内部两两树之间的相关性的平均水平
    * 根据各个随机向量独立同分布，可以推出 corelation 的第二个表达式，其中 ![](http://latex.codecogs.com/gif.latex?sd\left(\Theta\right)) is the standard deviation of ![](http://latex.codecogs.com/gif.latex?rmg\left(\Theta,X,Y\right)) holding ![](http://latex.codecogs.com/gif.latex?\Theta) fixed

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?\rho\left(\Theta,\Theta'\right)=cov_{X,Y}rmg\left(\Theta,X,Y\right)rmg\left(\Theta',X,Y\right)">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?\bar{\rho}=E_{\Theta,\Theta'}\left(\rho\left(\Theta,\Theta'\right)sd\left(\Theta\right)sd\left(\Theta'\right)\right)/E_{\Theta,\Theta'}\left(sd\left(\Theta\right)sd\left(\Theta'\right)\right)">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?\bar{\rho}=var\left(mr\right)/\left(E_{\Theta}sd\left(\Theta\right)\right)^{2}">
<p>
  
  * generalization error: 

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?PE^*=P_{X,Y}\left(mg\left(X,Y\right)<0)\right)"/>
<p>
  
  * strength: 对于每个森林，或者说是该森林对应的一系列树，基于强大数定理（具体可以参考 Leo 的论文），树足够多的情况下，所有树都在极限意义下等同于他们共同的分布 ![](http://latex.codecogs.com/gif.latex?\Theta)。
      * 下角标表示以 ![](http://latex.codecogs.com/gif.latex?\Theta) 为随机变量
      * 概率和之前 margin 定义中的 av 对应，效果相同
      * 对应的 margin function:    
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?mr\left(X,Y\right)=P_{\Theta}\left(h\left(X,\Theta\right)=Y\right)-max_{j\neq"/>
  <img src="http://latex.codecogs.com/gif.latex?_{Y}P_{\Theta}\left(h\left(X,\Theta\right)=j\right)"/>
<p>
  
  *
      * strength: 树分类器越强，strength数值对应越大。
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?s=E_{X,Y}mr\left(X,Y\right)"/>
<p>   
  
  * out-of-bag classifier: &nbsp; ![](http://latex.codecogs.com/gif.latex?h\left(x,T_{k}\right)) 
      *  *x* 为 input，原样本。
      *  ![](http://latex.codecogs.com/gif.latex?T_{k}) 为 bagging 得到的 training set，包含两个部分：bootstrap 得到的 sample 和 randomly selected features。classifier 表达式中应该（如果对 Leo 论文理解的没错的话）是特征随机选择方式。 
      *  该 classifier 是需要匹配对应的原样本的，每一个原样本点因为 bagging 约匹配所有 classifiers 中的 1/3。（这个思路个人感觉很不直观理解，不过用于进一步引出 generalization-error 还是必要的）

  * oob estimate of generalization error：error rate of the out-of-bag classifier on the training set
      * 对于每个分类器，用了 2/3 样本训练出来的分类器作用于未使用的 1/3 样本得到的分类错误率。
      * 利用 generalization error 来评价分类器。（模型 fitting 的过程最优地拟合了 training dataset，降低了估计的偏差；模型 cross-validation 的过程利用 ensemble 的模型降低了对于 validating dataset 的估计偏差）
  
  * oob estimate of strength
      *  estimate for ![](http://latex.codecogs.com/gif.latex?P_{\Theta}\left(h\left(x,\Theta\right)=y\right))
      *  不属于符号表示的部分对应 oob

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?Q\left(\mathbf{x},j\right)=\sum_{k}^{}I\left(h\left(\mathbf{x},\Theta_{k}\right)=j;\left(y,\mathbf{x}\right)\notin{T_{k,B}}\right)/\sum_{k}^{}I\left(\left(y,\mathbf{x}\right)\notin{T_{k,B}}\right)">
<p>
  
  *
    * strength 的估计接下来参照前文的表达式

  * oob estimate of correlation
    * 同样基于 estimate for ![](http://latex.codecogs.com/gif.latex?P_{\Theta}\left(h\left(x,\Theta\right)=y\right))
    * 使用上文中 correlation 的第二个表达式，各部分的估计方法如下（计算均为直接推导，没有任何技巧）
    * ![](http://latex.codecogs.com/gif.latex?p_{1},p_{2}) 也可以通过 ![](http://latex.codecogs.com/gif.latex?Q\left(x,j\right)) 算出

<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?var\left(mr\right)=\left(Q\left(x,y\right)-max_{j\neq{Y}}Q\left(x,j\right)\right)^{2}-s^{2}">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?sd\left(\Theta\right)=\left[p_{1}+p_{2}+\left(p_{1}-p_{2}\right)^{2}\right]^{1/2}">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?p_{1}=E_{X,Y}\left(h\left(X,\Theta\right)=Y\right)">
<p>
  
<p align="center">
  <img src="http://latex.codecogs.com/gif.latex?p_{2}=E_{X,Y}\left(h\left(X,\Theta\right)=\hat{j}\left(X,Y\right)\right)">
<p>
  

  ### 算法步骤
  1. 样本的构造：通过 bootstrap(bagging) 进行重抽样，来构造不同的训练集用于训练不同的决策树（分类器 classifier）。
      * 一个 bootstrap，N 个样本点，对应一颗树
      * bagging 最重要的作用：产生 oob estimates，进而可以进一步去估计 generalization error，strength，correlation，variable importance。
  3. 单个决策树的训练：每次使用部分特征量 m < M，用于决策树单个节点的训练，直到出现和父节点相同的特征，树没有修剪。
      * 多个 m 对应一棵树
  5. 重复 2 从而构建随机森林，使用所有树综合的（bagged）投票结果（votes）来作为最终的预测结果（predictor）。

 
[返回目录](#jump0)


## <span id="jump2">Maven 从已有项目生成原型 `mvn archetype:create-from-project`<span>
  
  ### 单模块
  
  * 编译原型文件
    ```shell
    mvn archetype:create-from-project
    ```
 * 切换至指定文件夹安装原型 jar/war 包
   ```shell
   cd target/generated-sources/archetype
   mvn install
   ```
   
  ### 多模块
  
  * 目前已知唯一区别于单模块的是：多模块项目经常会引用自身的子模块，所以基于原型生成项目过程需要注意 **groupId** 新项目和原型要一致
  
[返回目录](#jump0)

## <span id="jump3">修改大型项目的项目名、包名<span>
  
  ### 统一替换
  
  * 使用 `Ctrl+Shift+R` 根据需要选择范围以及字段匹配的要求，输入相应 groupId 和 artifactId 进行替换。
 
  ### 修改 package 和 module 名
  
  * 选中指定 package 和 module，右键 refactor -> rename，其中 package 改名会自动识别所有相同的 package 名，可选择批量修改。

[返回目录](#jump0)

## <span id="jump4">github 插入数学公式<span>
  
  ### [CodeCogs Equation Editor](https://latex.codecogs.com/)
  * 线上生成图片然后插入 `![](http://latex.codecogs.com/gif.latex?公式)`

[返回目录](#jump0)


## <span id="jump5">item5<span>
  
  ### item5.1
 
  ### item5.2
  
[返回目录](#jump0)

