## <span id="jump0">目录<span>
  
  * [SVM](#jump1)
  * [A balance between *p*(parameters/features) and *n*(samples)](#jump2)
  * [Mathematical Optimization](#jump3)
  * [Quadratic programming](#jump4)
  * [item5](#jump5)

## <span id="jump1">SVM<span>
  
  ### [SVM wiki](https://en.wikipedia.org/wiki/Support-vector_machine)

  ### Motivation
  
  * Following machine learning insight, the goal is to predict based on similarity. 
  * The idea to design the weights is by seperate the samples with the **maximum-margin hyperplane**.
  * Easy to find out that this method is based on **computing the distances** between margin and samples considering attaining the **best seperation**(maximum margin or the most fair margin for two or more groups).
 
  ### Then
  
  * The simiplest margin type is the linear one. Ya, the linear SVM, in which the hyperplane is a line, u can eventually find a angle to see a line.
  * So it'll be nice if we can all achieve a general linear SVM somehow after u tried to solve the optimal problem in nonlinear case.(Yes, it's computational complex. Linear case is easy to do) Then comes the kernel trick.
  
  ### How hard it is?
  
  * When computing distance in linear case, u notice some good properties.
    * the hyperplane can be represented as ![](http://latex.codecogs.com/gif.latex?W^{T}x-b=0)
    * the distance between two generated hyperplanes(the middleware to achieve the maximum-margin) and the middle hyperplane is ![](http://latex.codecogs.com/gif.latex?\frac{1}{\left|\left|w\right|\right|})
    * yes, it's not strange u can attain similar results in nonlinear case. But the computation is probably complex. And now we solve the linear case first.
  * Here comes a branch since sometimes the orginal idea(a perfect but strict seperation) is not achievable. Then we have hard-marigin and soft-margin with different loss function.
    * hard-marigin

<p align="center">
  <img src=>

  ### So how to make it easy?
  
  * While every **dot product**(needed when computing distances) is replaced by a nonlinear **kernel function**(essentially using a new similarity measure), a linear SVM is achieved following the original algorithm(essentially transforming the feature space into a new one).
  
  ### What's more

  
[返回目录](#jump0)


## <span id="jump2">A balance between *p*(parameters/features) and *n*(samples)<span>
  
  ### Motivation
  
  * Noticed this topic after reading the general advices for choosing kernel functions giving different numbers of *p*(number of features) and *n*(samples).
  * Generally
    * *p* is the ability of capturing information.
    * *n* is the ability of providing information.
  * The best scenario is when u can caputre adequate(a *p* big enough) information from a information pool of adequate(a *n* big enough) information
 
  ### Method
  
  * *n* means more sample
  * *p* means constructing more features
  
 
[返回目录](#jump0)

## <span id="jump3">Mathematical Optimization<span>
  
  ### [Mathematical Optimization wiki](https://en.wikipedia.org/wiki/Mathematical_optimization#Major_subfields)


[返回目录](#jump0)

## <span id="jump4">Quadratic programming<span>
  
  ### [Quadratic programming wiki](https://en.wikipedia.org/wiki/Quadratic_programming)
  
  * a multivariate quadratic objective function
  * linear constraints

<p align="center">
  <img src=http://latex.codecogs.com/gif.latex?\frac{1}{2}x^{T}Qx+c^{T}x>
<p>
  
<p align="center">
  <img src=http://latex.codecogs.com/gif.latex?s.t.Ax\leqslant{b}>
<p>
 
  ### Solution methods
  
  #### extensions of the simplex algorithm
  
  * [Simplex algorithm](https://en.wikipedia.org/wiki/Simplex_algorithm) for linear programming

<p align="center">
  <img src=http://latex.codecogs.com/gif.latex?max_{}c^{T}x>
<p>  
<p align="center">
  <img src=http://latex.codecogs.com/gif.latex?s.t.Ax\leqslant{b}>
<p>  
<p align="center">
  <img src=http://latex.codecogs.com/gif.latex?for\forall{i},x_{i}\geqslant{0}>
<p>
  
  
  * Insight of the algorithm: find a suboptimal solution, then find the optimal solution.
  * Understanding linear programming from a geometric view: the feasible region is a convex polytope.
    * each vertex is a local optimal solution in the sense of the edge between.(because of conven, following one edge, the objective goes monotonously)
    * jumping between vertices by following edges shows a way of finding a smaller objective function(a better result).
  * Algorithm:
    * step 1: choose a starting vertex through the constraints.(The intercept of two edges)
    * step 2: start from the vertex from step 1 and jump to a better place.(following the rule above)

<p align="center">
  <img src=https://github.com/mylu314/blog/blob/main/images/Simplex-method-3-dimensions.png width=200>
<p>
  
  * what's more
    * sometimes efficient
    * sometimes NP-mighty
    * a variant is proved to be NP-mighty

[返回目录](#jump0)


## <span id="jump5">item5<span>
  
  ### item5.1
 
  ### item5.2
  
[返回目录](#jump0)
