## <span id="jump0">目录<span>
  
  * [SVM](#jump1)
  * [item2](#jump2)
  * [item3](#jump3)
  * [item4](#jump4)
  * [item5](#jump5)

## <span id="jump1">SVM<span>

  ### Motivation
  
  * Following machine learning idea, the goal is to predict based on similarity. 
  * The idea to design the weights is by seperate the samples with the **maximum-margin hyperplane**.
  * Easy to find out that this method is based on **computing the distances** between margin and samples considering attaining the **best seperation**(maximum margin or the most fair margin for two or more groups).
 
  ### Then
  
  * The simiplest margin type is the linear one. Ya, the linear SVM, in which the hyperplane is a line, u can eventually find a angle to see a line.
  * So it'll be nice if we can all achieve a general linear SVM somehow after u tried to solve the optimal problem in nonlinear case.(Yes, it's computational complex. Linear case is easy to do) Then comes the kernel trick.
  * While every **dot product**(needed when computing distances) is replaced by a nonlinear **kernel function**(essentially using a new similarity measure), a linear SVM is achieved following the original algorithm(essentially transforming the feature space into a new one).
  
  ### What's more
  
  * Maybe sometimes we need a less strict rule. Then comes two marigin types: hard-marigin and soft-margin.
  * 
  
[返回目录](#jump0)


## <span id="jump2">item2<span>
  
  ### item2.1
 
  ### item2.2
  
 
[返回目录](#jump0)

## <span id="jump3">item3<span>
  
  ### item3.1
 
  ### item3.2

[返回目录](#jump0)

## <span id="jump4">item4<span>
  
  ### item4.1
 
  ### item4.2

[返回目录](#jump0)


## <span id="jump5">item5<span>
  
  ### item5.1
 
  ### item5.2
  
[返回目录](#jump0)
